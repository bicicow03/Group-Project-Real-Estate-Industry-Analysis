{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d663f90",
   "metadata": {},
   "source": [
    "# MAST30034 Project 2 Group 1 Summary Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7634c7",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271a15c",
   "metadata": {},
   "source": [
    "### Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24abd3",
   "metadata": {},
   "source": [
    "#### External datasets\n",
    "- **ABS Census data:**\n",
    "- **Crime data:**\n",
    "- **Population data:**\n",
    "- **Public transport data:** Downloaded from PTV General Transit Feed Specification (GTFS) Data. As of September 2025.\n",
    "- **School data:**\n",
    "\n",
    "#### Given datasets\n",
    "- **Rental property listings:** Scraped from domain.com.au. As of September 2025.\n",
    "- **Moving annual median rent by suburb and town:** Victoria Government Families, Fairness and Housings Rental report. As of March quarter 2025.\n",
    "\n",
    "**Assumptions and limitations:**\n",
    "\n",
    "The rental dataset reflects only observed/scraped listings and may not necessarily be representative of all rental properties available within Victoria.\n",
    "Additionally, using several external datasets (resulting in 57 total features) may introduce potential multicollinearity between features (e.g. number of schools vs suburb population). Thus, the model may capture correlations rather than causal effects. However, we aim to address this through feature selection and feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f21f9d",
   "metadata": {},
   "source": [
    "## 2. Dataset building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e6810",
   "metadata": {},
   "source": [
    "The 5 external datasets were processed and cleaned to be merged with the rental property listings dataset.\n",
    "\n",
    "This means that for each rental listing, there will be rental specific attributes (e.g. number of bedrooms, number of schools within 2km), as well as suburb level attributes (e.g. suburb population, suburb crime index).\n",
    "\n",
    "This gives us 57 initial attributes/features, which need to be further cleaned, processed, filtered and selected to analyse rental prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2c06a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['listing_id', 'suburb', 'postcode', 'weekly_rent', 'bond',\n",
       "       'available_date', 'date_listed', 'days_listed', 'bedrooms', 'bathrooms',\n",
       "       'carspaces', 'property_type', 'address', 'lat', 'lon', 'photo_count',\n",
       "       'video_count', 'floorplans_count', 'virtual_tour', 'primary_type',\n",
       "       'secondary_type', 'agency', 'agent_names', 'land_area',\n",
       "       'num_metro_bus_stops', 'num_metro_tram_stops', 'num_metro_train_stops',\n",
       "       'num_regional_bus_stops', 'num_regional_train_stops', 'num_schools_2km',\n",
       "       'Median_age_persons', 'Median_mortgage_repay_monthly',\n",
       "       'Median_tot_prsnl_inc_weekly', 'Median_rent_weekly',\n",
       "       'Median_tot_fam_inc_weekly', 'Average_num_psns_per_bedroom',\n",
       "       'Median_tot_hhd_inc_weekly', 'Average_household_size',\n",
       "       'Owner occupied (%)', 'Mortgage (%)', 'Total rented (%)',\n",
       "       'Other tenure (%)', 'Unemployment', 'post_gradutae (%)',\n",
       "       'Graduate_diploma_certificate(%)', 'Bachelor (%)',\n",
       "       'Advanced_&_Diploma (%)', 'Certificate_level (%)', 'Total_persons',\n",
       "       'Population-2023', 'SAL_NAME21', 'incidents_recorded',\n",
       "       'rate_per_100000_population', 'population_est', 'crime_per_person',\n",
       "       'crime_index', 'crime_rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "merged_dataset = pd.read_csv(\"../data/processed/real_estate/vic_rentals_all_enriched.csv\")\n",
    "merged_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a976de9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebdd3d",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee1861",
   "metadata": {},
   "source": [
    "With the initial merged dataset, need to deal with any possible nan values per feature. This follows a 2 step process:\n",
    "\n",
    "**1. Mean imputation:** We create a lookup dictionary that is grouped by the property's suburb, property_type, bedrooms, and bathrooms and impute the feature's missing value according to the mean-aggregated dictionary value. Next, we create a relaxed version of this lookup dictionary on 'property_type', 'bedrooms' and use a similar pattern to impute more nans. This is done for the \"weekly_rent\" and \"carspaces\" features, which have the among the highest missing values. \n",
    "\n",
    "**2. Listwise deletion:** After imputing nans, there is significantly less remaining missing values so just drop them.\n",
    "\n",
    "Note: land_area was simply dropped and remove despite being a potential useful feature as it had too many missing values i.e 12329/12331\n",
    "\n",
    "**Assumptions and limitations**\n",
    "- Assumes features are missing at random and not systematically biased\n",
    "- Mean imputation may be too simple a method for imputation so may ignore natural variance in features. Though this is aimed to be addressed through the use of lookup dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d071146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nans(data):\n",
    "    missing_list = [(col, data[col].isnull().sum()) for col in data.columns]\n",
    "    non_nans = [(col, cnt) for col, cnt in missing_list if cnt != 0]\n",
    "    return sorted(non_nans, key=lambda x: x[1], reverse=True)  # sort by column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec057388",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mland_area\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAL_NAME21\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuburb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbond\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(find_nans(data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = data.drop(columns=[\"land_area\", \"SAL_NAME21\", 'suburb', 'bond'])\n",
    "print(find_nans(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23675260",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daba01e",
   "metadata": {},
   "source": [
    "These numerical features were considered to determine outliers: ['weekly_rent', 'bedrooms', 'bathrooms', 'carspaces', 'num_metro_bus_stops', 'num_metro_tram_stops', 'num_schools_2km', 'incidents_recorded']\n",
    "\n",
    "We assume that extreme values are unrepresentative, which should be a valid assumption as there are only 27 rental properties above $3000, which should have little impact on model performance.\n",
    "\n",
    "We used $3000 as the as upper limit for the rental prices of houses in the Vic-Gov website is $2885. (https://www.housing.vic.gov.au/what-does-rent-cost-victoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8912ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero rent count: 16\n",
      "High outlier rent count: 27\n",
      "High bedroom count: 1\n"
     ]
    }
   ],
   "source": [
    "#Find how many 0 weekly_rent values there are\n",
    "zero_rent_count = (data[\"weekly_rent\"] == 0).sum()\n",
    "print(\"Zero rent count:\", zero_rent_count)\n",
    "\n",
    "#Find how many high outlier weekly_rent values there are i.e above 3000\n",
    "highoutlier_rent_count = (data[\"weekly_rent\"] >= 3000).sum()\n",
    "print(\"High outlier rent count:\", highoutlier_rent_count)\n",
    "\n",
    "#Find how many data points with 50 or more bedrooms\n",
    "high_bedroom_count = (data[\"bedrooms\"] >= 50).sum()\n",
    "print(\"High bedroom count:\", high_bedroom_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b05d2",
   "metadata": {},
   "source": [
    "Surprisingly, not many outliers were detected. Removing these outliers gives the following distribution for the numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c9df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekly_rent</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>carspaces</th>\n",
       "      <th>num_metro_bus_stops</th>\n",
       "      <th>num_metro_tram_stops</th>\n",
       "      <th>num_schools_2km</th>\n",
       "      <th>incidents_recorded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "      <td>12018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>621.903977</td>\n",
       "      <td>2.720835</td>\n",
       "      <td>1.587203</td>\n",
       "      <td>1.626061</td>\n",
       "      <td>61.837910</td>\n",
       "      <td>20.948910</td>\n",
       "      <td>8.061075</td>\n",
       "      <td>13259.928384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>249.176926</td>\n",
       "      <td>1.081993</td>\n",
       "      <td>0.629443</td>\n",
       "      <td>0.946937</td>\n",
       "      <td>43.221829</td>\n",
       "      <td>35.017988</td>\n",
       "      <td>4.798088</td>\n",
       "      <td>5828.544925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>490.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9525.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>560.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13140.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>683.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17495.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>34620.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weekly_rent      bedrooms     bathrooms     carspaces  \\\n",
       "count  12018.000000  12018.000000  12018.000000  12018.000000   \n",
       "mean     621.903977      2.720835      1.587203      1.626061   \n",
       "std      249.176926      1.081993      0.629443      0.946937   \n",
       "min       33.000000      1.000000      1.000000      1.000000   \n",
       "25%      490.000000      2.000000      1.000000      1.000000   \n",
       "50%      560.000000      3.000000      2.000000      1.000000   \n",
       "75%      683.000000      4.000000      2.000000      2.000000   \n",
       "max     3000.000000     11.000000     12.000000     22.000000   \n",
       "\n",
       "       num_metro_bus_stops  num_metro_tram_stops  num_schools_2km  \\\n",
       "count         12018.000000          12018.000000     12018.000000   \n",
       "mean             61.837910             20.948910         8.061075   \n",
       "std              43.221829             35.017988         4.798088   \n",
       "min               0.000000              0.000000         0.000000   \n",
       "25%              22.000000              0.000000         4.000000   \n",
       "50%              66.000000              0.000000         8.000000   \n",
       "75%              96.000000             35.000000        12.000000   \n",
       "max             183.000000            127.000000        23.000000   \n",
       "\n",
       "       incidents_recorded  \n",
       "count        12018.000000  \n",
       "mean         13259.928384  \n",
       "std           5828.544925  \n",
       "min             77.000000  \n",
       "25%           9525.000000  \n",
       "50%          13140.500000  \n",
       "75%          17495.333333  \n",
       "max          34620.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Remove outliers rows\n",
    "data = data[(data[\"weekly_rent\"] > 0) & (data[\"weekly_rent\"] <= 3000) & (data[\"bedrooms\"] < 50)]\n",
    "#Looking at numerical variables\n",
    "data[['weekly_rent', 'bedrooms', 'bathrooms', 'carspaces', 'num_metro_bus_stops', 'num_metro_tram_stops', 'num_schools_2km', 'incidents_recorded']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fff95",
   "metadata": {},
   "source": [
    "Finally, after encoding categorical variables (extracting time features from 'available_date'), our dataset is finally cleaned and ready to be analysed to determine important features for predicting rental prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4514b8f",
   "metadata": {},
   "source": [
    "## 4. Modelling Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e2ee8",
   "metadata": {},
   "source": [
    "Models: Random Forest Regressor and GX boost were selected with their ability to capture complex relationships in data and a useful feature importance function to help understand which features are were most important in model predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151783",
   "metadata": {},
   "source": [
    "### Feature Engeering And Encoding\n",
    "\n",
    "Time data was feature engineered to hour, day and month; and then encoded using cyclic encoding to help model capture potential seasonal change in rent prices.\n",
    "\n",
    "Frequency encoding was used for non-numerical features i.e postcode, property_type and agency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3221a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering time data\n",
    "data['available_date'] = pd.to_datetime(data['available_date'], errors='coerce')\n",
    "data['available_day'] = data['available_date'].dt.day\n",
    "data['available_month'] = data['available_date'].dt.month   \n",
    "data['available_year'] = data['available_date'].dt.year\n",
    "data = data.drop(columns=['available_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode month cyclically \n",
    "data['month_sin'] = np.sin(data['available_month'] / 12 * 2 * np.pi)\n",
    "data['month_cos'] = np.cos(data['available_month'] / 12 * 2 * np.pi)\n",
    "data = data.drop(columns=['available_month'])\n",
    "\n",
    "#Encode day cyclically\n",
    "data['day_sin'] = np.sin(data['available_day'] / 31 * 2 * np.pi)\n",
    "data['day_cos'] = np.cos(data['available_day'] / 31 * 2 * np.pi)\n",
    "data = data.drop(columns=['available_day'])\n",
    "\n",
    "#Frequency encoding for Non-numericeal columns\n",
    "post_freq = data['postcode'].value_counts(normalize=True)\n",
    "data['postcode'] = data['postcode'].map(post_freq)\n",
    "property_freq = data['property_type'].value_counts(normalize=True)\n",
    "data['property_type'] = data['property_type'].map(property_freq)\n",
    "agency_freq = data['agency'].value_counts(normalize=True)\n",
    "data['agency'] = data['agency'].map(agency_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0ac75",
   "metadata": {},
   "source": [
    "#### Limitations and Assumptions\n",
    "\n",
    "We assume that postcode, propety_type and agency have certain catergories(e.g a common postcode, popular property type,etc.) that can influence weekly rent prices. \n",
    "\n",
    "Cycle encodinng treats and months and day pattern as if patterns always repeat identically, training only on 2025 might cause bias in the predictions of data for a different year. This shouldn't be an issue as this data set will not be used in forcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43161202",
   "metadata": {},
   "source": [
    "### Data leakage\n",
    "\n",
    "Certain features in the data such as 'median_rent_weekly', 'median_morgage_repay_monthly' and 'bond' can cause data leakage and hence removed. (bond was remove earlier in preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f503913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop lat and long for modeling\n",
    "data = data.drop(columns=['lat', 'lon', 'Median_rent_weekly', 'Median_mortgage_repay_monthly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d4414",
   "metadata": {},
   "source": [
    "All data was rescaled using the Standardize Scalar method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e73e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize scalar, resacling all data. (can target specific columns if needed)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ce8d3",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe8a13",
   "metadata": {},
   "source": [
    "Feature Selection was conducted using Mutual Information(MI) as we hav 42 features. 20 Features were selected from the 42.\n",
    "\n",
    "Results show that models without feature selection performes significantly better with average r^2 of 0.7 and average MAE of 70 compared to mdoels with feature selection with average r^2  0.25 and average MAE of 126.5.\n",
    "\n",
    "Cause: likely due to MI only capturing univeriate relationships between one variable vs target variable while our data contains strong multivariable interactions; hence MI fails to compare them and discards useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MI\n",
    "mi = mutual_info_regression(X, y, discrete_features=\"auto\", random_state=0)\n",
    "mi_scores = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "k = 20\n",
    "selected_features = mi_scores.head(k).index.tolist()\n",
    "\n",
    "X_selection = X[selected_features]\n",
    "X_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90d331",
   "metadata": {},
   "source": [
    "## 5. Liveability & affordability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a9d41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Components that go into livability score of a suburb are: \n",
    "- number of schools  \n",
    "- number of train, bus and tram stops \n",
    "- crime index as normalised (0-100) score of total offenses recorded within the suburb, where below 20 is considered low crime\n",
    "\n",
    "To quanitfy the livability we assigned weights to the components above, the assumption was that crime is the most important indicator of suburb's quality hence its weight of 0.4. Number of schools is the second most important factor with weight 0.3, then number of public transport stops with overall weight of 0.3. \n",
    "\n",
    "### Afordability calculation \n",
    "\n",
    "Affordability was quantified as ratio of weekly rent to median weekly household income in a suburb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b37076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most livable (postcode proxy):\n",
      " postcode  livability_score  livability_rank  affordability_score\n",
      "     3126          0.733811              1.0             0.745382\n",
      "     3123          0.731158              2.0             0.792613\n",
      "     3183          0.717356              3.0             0.765768\n",
      "     3162          0.712354              4.0             0.759937\n",
      "     3068          0.688550              5.0             0.732063\n",
      "     3185          0.673369              6.0             0.777010\n",
      "     3122          0.666856              7.0             0.767968\n",
      "     3144          0.656467              8.0             0.732655\n",
      "     3204          0.655334              9.0             0.684032\n",
      "     3103          0.654143             10.0             0.680877\n",
      "     3189          0.654015             11.0             0.724151\n",
      "     3142          0.653293             12.0             0.748000\n",
      "     3124          0.652378             13.0             0.771147\n",
      "     3146          0.651324             14.0             0.778085\n",
      "     3039          0.639298             15.0             0.800453\n",
      "     3070          0.635828             16.0             0.763269\n",
      "     3071          0.632504             17.0             0.753102\n",
      "     3040          0.631848             18.0             0.794098\n",
      "     3057          0.631839             19.0             0.767753\n",
      "     3187          0.627888             20.0             0.642080\n",
      "\n",
      "Top 20 most affordable (postcode proxy):\n",
      " postcode  affordability_score  affordability_rank  livability_score\n",
      "     3987             1.000000                 1.0          0.333268\n",
      "     3761             0.894657                 2.0          0.399438\n",
      "     3641             0.875226                 3.0          0.331038\n",
      "     3160             0.874733                 4.0          0.480024\n",
      "     3345             0.863322                 5.0          0.362961\n",
      "     3735             0.860715                 6.0          0.358324\n",
      "     3438             0.847740                 7.0          0.413154\n",
      "     3329             0.845014                 8.0          0.415789\n",
      "     3795             0.842395                 9.0          0.382847\n",
      "     3078             0.834569                10.0          0.469391\n",
      "     3766             0.833516                11.0          0.395505\n",
      "     3821             0.833245                12.0          0.312974\n",
      "     3537             0.833196                13.0          0.348606\n",
      "     3331             0.830508                14.0          0.431579\n",
      "     3437             0.828377                15.0          0.363516\n",
      "     3335             0.827064                16.0          0.351388\n",
      "     3016             0.826981                17.0          0.456499\n",
      "     3357             0.825792                18.0          0.305950\n",
      "     3756             0.823520                19.0          0.360972\n",
      "     3102             0.819339                20.0          0.479696\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_affordable\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cm, colors\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Load the shapefile for postcode boundaries\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load the enriched suburbs dataset\n",
    "df = pd.read_csv('../data/curated/cleaned_real_estate_data.csv')\n",
    "df.head()\n",
    "\n",
    "# Aggregate to postcode level (use median for rents/coords, median/mean for counts)\n",
    "agg = df.groupby('postcode').agg(\n",
    "    median_tot_hhd_inc_weekly = ('Median_tot_hhd_inc_weekly','median'),\n",
    "    num_schools_2km = ('num_schools_2km', 'median'),\n",
    "    num_metro_train_stops = ('num_metro_train_stops', 'median'),\n",
    "    num_metro_tram_stops = ('num_metro_tram_stops', 'median'),\n",
    "    num_metro_bus_stops = ('num_metro_bus_stops', 'median'),\n",
    "    crime_index = ('crime_index', 'median'),\n",
    "    weekly_rent = ('weekly_rent', 'median'),\n",
    "    lat = ('lat', 'median'),\n",
    "    lon = ('lon', 'median'),\n",
    "    count_rows = ('postcode', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Scale features (higher = better). Invert crime & rent.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def norm(series):\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1)).ravel()\n",
    "\n",
    "# Livability components (higher = better). Invert crime.\n",
    "agg['schools_norm'] = norm(agg['num_schools_2km'])\n",
    "agg['train_norm']   = norm(agg['num_metro_train_stops'])\n",
    "agg['tram_norm']    = norm(agg['num_metro_tram_stops'])\n",
    "agg['bus_norm']     = norm(agg['num_metro_bus_stops'])\n",
    "agg['crime_norm']   = 1 - norm(agg['crime_index'])\n",
    "\n",
    "\n",
    "# Composite scores (chosen metrics / weights)\n",
    "# Livability weights: schools 0.30, train 0.10, tram 0.10, bus 0.10, crime 0.40\n",
    "agg['livability_score'] = (\n",
    "    0.30 * agg['schools_norm'] +\n",
    "    0.10 * agg['train_norm'] +\n",
    "    0.10 * agg['tram_norm'] +\n",
    "    0.10 * agg['bus_norm'] +\n",
    "    0.40 * agg['crime_norm']\n",
    ")\n",
    "\n",
    "# Affordability using median_tot_hhd_inc_weekly:\n",
    "# rent_to_income = weekly_rent / median_tot_hhd_inc_weekly (lower = more affordable)\n",
    "agg['rent_to_income'] = agg['weekly_rent'] / agg['median_tot_hhd_inc_weekly']\n",
    "agg['affordability_score'] = 1 - norm(agg['rent_to_income'])  # higher = more affordable\n",
    "\n",
    "# Ranks\n",
    "agg['livability_rank'] = agg['livability_score'].rank(ascending=False, method='min')\n",
    "agg['affordability_rank'] = agg['affordability_score'].rank(ascending=False, method='min')\n",
    "\n",
    "# Top lists\n",
    "top_livable = agg.sort_values('livability_score', ascending=False).head(20)[\n",
    "    ['postcode','livability_score','livability_rank','affordability_score']\n",
    "]\n",
    "top_affordable = agg.sort_values('affordability_score', ascending=False).head(20)[\n",
    "    ['postcode','affordability_score','affordability_rank','livability_score']\n",
    "]\n",
    "\n",
    "print(\"Top 20 most livable (postcode proxy):\")\n",
    "print(top_livable.to_string(index=False))\n",
    "print(\"\\nTop 20 most affordable (postcode proxy):\")\n",
    "print(top_affordable.to_string(index=False))\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from matplotlib import cm, colors\n",
    "\n",
    "# Load the shapefile for postcode boundaries\n",
    "postcode_gdf = gpd.read_file('../../data/landing/boundaries/SA2_2021_AUST_GDA2020.shp')\n",
    "\n",
    "# Center on Victoria (approximate center)\n",
    "vic_center = [-37.0, 144.5]\n",
    "m_livability = folium.Map(location=vic_center, zoom_start=7, tiles='cartodbpositron')\n",
    "m_affordability = folium.Map(location=vic_center, zoom_start=7, tiles='cartodbpositron')\n",
    "\n",
    "# Select top 20 only\n",
    "top_n = 20\n",
    "top_livable = agg.nlargest(top_n, 'livability_score').reset_index(drop=True)\n",
    "top_affordable = agg.nlargest(top_n, 'affordability_score').reset_index(drop=True)\n",
    "\n",
    "# Colormaps and normalization (use top-20 range so colours are meaningful)\n",
    "cmap_liv = cm.get_cmap('YlGn')\n",
    "cmap_aff = cm.get_cmap('YlOrRd')\n",
    "\n",
    "liv_min, liv_max = top_livable['livability_score'].min(), top_livable['livability_score'].max()\n",
    "aff_min, aff_max = top_affordable['affordability_score'].min(), top_affordable['affordability_score'].max()\n",
    "\n",
    "# Plot top 20 livable\n",
    "for i, row in top_livable.iterrows():\n",
    "    if np.isnan(row['lat']) or np.isnan(row['lon']):\n",
    "        continue\n",
    "    val = 0.5\n",
    "    if liv_max > liv_min:\n",
    "        val = (row['livability_score'] - liv_min) / (liv_max - liv_min)\n",
    "    hexcol = colors.to_hex(cmap_liv(val))\n",
    "    radius = 7\n",
    "    # highlight the best (first row from nlargest)\n",
    "    if i == 0:\n",
    "        radius = 12\n",
    "        hexcol = '#0000FF'  # blue for best\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=radius,\n",
    "        fill=True,\n",
    "        fill_opacity=0.9,\n",
    "        color=hexcol,\n",
    "        fill_color=hexcol,\n",
    "        tooltip=f\"Postcode: {row['postcode']}<br>Livability: {row['livability_score']:.3f}<br>Rank: {int(row['livability_rank']) if 'livability_rank' in row else 'N/A'}\"\n",
    "    ).add_to(m_livability)\n",
    "    \n",
    "display(m_livability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260dbc8",
   "metadata": {},
   "source": [
    "Models \n",
    "Key features: bathrooms, bedrooms and Bachelor (%) were signficantly more important than every other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060446b5",
   "metadata": {},
   "source": [
    "## 5. Rent Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74f8b7",
   "metadata": {},
   "source": [
    "### Findings \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "houses = pd.read_csv(\"../data/outputs/houses_growth.csv\")\n",
    "\n",
    "units = pd.read_csv(\"../data/outputs/apartments_growth.csv\")\n",
    "\n",
    "overall = pd.read_csv(\"../data/outputs/overall_growth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ccb3df",
   "metadata": {},
   "source": [
    "## 6. Limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
